\relax 
\bibstyle{agsm}
\citation{Ofcom}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction - 2/3 pages}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Context}{1}}
\citation{Ofcom}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Objectives}{2}}
\citation{Reynolds}
\citation{Reynolds}
\citation{Dixon}
\citation{Golbeck}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work - 2-4 pages}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Definitions of subjective terms}}{3}}
\newlabel{terms}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Machine Learning}{3}}
\citation{Hack}
\citation{Birds}
\citation{Hack}
\citation{Badjatiya}
\citation{Badjatiya}
\citation{Badjatiya}
\citation{Hack}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Deep Learning}{4}}
\citation{Golbeck}
\citation{Dixon}
\citation{Waseem}
\citation{Badjatiya}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Datasets}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Public datasets used in this project}}{5}}
\newlabel{data}{{2}{5}}
\citation{Reynolds}
\@writefile{toc}{\contentsline {section}{\numberline {III}Solution - 4/7 pages}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Tools Used}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Pre-processing the data}{6}}
\citation{glove}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Input words with GloVe representations, before and after data cleaning.}}{7}}
\newlabel{glove}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Feature Extraction}{7}}
\citation{DL}
\citation{glove}
\citation{Peters}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Word Embeddings}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}ELMo}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Other Data Considerations}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Length distribution of data: \textbf  {Left}: Reddit, \textbf  {Centre}: Twitter\_small, \textbf  {Right}: Twitter\_big}}{9}}
\newlabel{len:len3}{{1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G}Machine Learning methods}{9}}
\citation{DL}
\citation{DL}
\@writefile{toc}{\contentsline {subsection}{\numberline {H}Deep Learning methods}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {H.1}Recurrent Neural Networks (RNNs)}{10}}
\newlabel{RNN}{{H.1}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {RNNs. $h$=state, $x$=input, $y$=gold label, $\sigma (T)$=predicted label, $t$=time/index}}}{10}}
\citation{DL}
\citation{DL}
\citation{Kim}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {H.2}Convolutional Neural Networks (CNNs)}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {H.3}Loss functions}{11}}
\citation{DL}
\citation{Sergey}
\newlabel{multichannel}{{H.2}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Multi-channel CNN. NOTE: I apply sigmoid for the 2-class problem, not softmax as in this diagram.}}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {H.4}Regularization}{12}}
\citation{Badjatiya}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results - 2/3 pages}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}F1 score}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Training curves}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Example training graph, LSTM on \textbf  {Twitter\_small} dataset}}}{13}}
\newlabel{training}{{4}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Final results}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Results on \textbf  {Reddit} dataset}}{14}}
\newlabel{results1}{{4}{14}}
\citation{Hack}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Results on \textbf  {Twitter\_small} dataset}}{15}}
\newlabel{results2}{{5}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Results on \textbf  {Twitter\_big\_2class} dataset}}{15}}
\newlabel{results3}{{6}{15}}
\citation{Badjatiya}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Binary Cross-Entropy Loss vs F1 loss}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Binary Cross-Entropy vs F1 loss function}}{16}}
\newlabel{F1Loss}{{7}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training curves on \textbf  {Twitter\_big\_2class}. \textbf  {Left:} Cross-Entropy loss, \textbf  {Right:} F1 loss.}}{16}}
\newlabel{train:train2}{{5}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}3-class problem}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Results on \textbf  {Twitter\_big\_3class} dataset}}{17}}
\newlabel{results4}{{8}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Confusion matrices. \newline  \textbf  {Left:} 2-class problem, \textbf  {Right:} 3-class problem (0=None, 1=Racism, 2=Sexism)}}{17}}
\newlabel{CM:CM2}{{6}{17}}
\citation{Hack}
\citation{Badjatiya}
\citation{BuildDeeper}
\@writefile{toc}{\contentsline {section}{\numberline {V}Evaluation - 2/3 pages}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Strengths and Limitations}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Is Deep Learning worth it?}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}The Importance of Data}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {The importance of dataset size for ML and DL performance.}}}{19}}
\newlabel{MLvsDL}{{7}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Future work}{19}}
\bibdata{projectpaper}
\harvardcite{BuildDeeper}{Amaratunga}{Amaratunga}{2019}
\harvardcite{Badjatiya}{Badjatiya, Gupta, Gupta \harvardand \^^MVarma}{Badjatiya et~al.}{2017}
\harvardcite{Hack}{Bastidas, Dixon, Loo \harvardand \^^MRyan}{Bastidas et~al.}{2016}
\harvardcite{Birds}{Chatzakou, Kourtellis, Blackburn, Cristofaro, Stringhini \harvardand \ Vakali}{Chatzakou et~al.}{2017}
\harvardcite{Dixon}{Dixon}{Dixon}{2018}
\harvardcite{Golbeck}{Golbeck}{Golbeck}{2018}
\harvardcite{DL}{Goodfellow, Bengio \harvardand \^^MCourville}{Goodfellow et~al.}{2016}
\harvardcite{Sergey}{Ioffe \harvardand \ Szegedy}{Ioffe \harvardand \ Szegedy}{2015}
\harvardcite{Kim}{Kim}{Kim}{2014}
\harvardcite{Ofcom}{Ofcom}{Ofcom}{2017}
\harvardcite{glove}{Pennington, Socher \harvardand \^^MManning}{Pennington et~al.}{2014}
\harvardcite{Peters}{Peters, Neumann, Iyyer, Gardner, Clark, Lee \harvardand \ Zettlemoyer}{Peters et~al.}{2018}
\harvardcite{Reynolds}{Reynolds \harvardand \ Kontostathis}{Reynolds \harvardand \ Kontostathis}{2011}
\harvardcite{Waseem}{Waseem \harvardand \ Hovy}{Waseem \harvardand \ Hovy}{2016}
